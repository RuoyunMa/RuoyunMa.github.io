---
title: "Action Recognition in RGB-D Egocentric Videos"
collection: publications
permalink: /publication/RDEV
date: 2017-05-09
venue: 'ICIP\'17'
citation: 'Y. Tang, Y. Tian, J. Lu, J. Feng and J. Zhou, "Action recognition in RGB-D egocentric videos," 2017 IEEE International Conference on Image Processing (ICIP), Beijing, 2017, pp. 3410-3414.'
---
[[IEEExplore]](https://ieeexplore.ieee.org/document/8296915/) [[pdf]](http://ruoyunma.github.io/files/RDEV.pdf)

## Abstract

 In this paper, we investigate the problem of action recognition in 
RGB-D egocentric videos. These self-generated and embodied videos 
provide richer semantic cues than the conventional videos captured from 
the third-person view for action recognition. Moreover, they contain 
both appearance information and 3D structure of the scenes from the RGB 
modality and depth modality respectively. Motivated by these advantages,
 we first collect a video-based RGB-D egocentric dataset (THU-READ) with
 diverse types of daily-life actions. Then we evaluate several 
approaches including hand-crafted features and deep learning methods on 
THU-READ. To improve the performance, we further develop a tri-stream 
convolutional network (TCNet) method, which learns to exploit the fuse 
with both the RGB and depth modalities for action recognition. 
Experimental results show that our model achieves competitive 
performance with state-of-the-art methods.
